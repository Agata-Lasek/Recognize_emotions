const textDisplay = document.getElementById('text-display');
const emotionDisplay = document.getElementById('emotion-display');
const recordButton = document.getElementById('record-button');
const stopButton = document.getElementById('stop-button');
const audioPlayer = document.getElementById('audio-player');
const audioSource = document.getElementById('audio-source');
const waveform = document.getElementById('waveform');

let mediaRecorder;
let audioChunks = [];
let animationFrameId;

function setAudioPlayer(filePath) {
    const audioPlayer = document.getElementById('audio-player');
    const audioSource = document.getElementById('audio-source');

    // Ustaw ≈∫r√≥d≈Ço pliku
    audioSource.src = filePath;

    // Za≈Çaduj nowe dane do odtwarzacza
    audioPlayer.load();

    // Wy≈õwietl kontrolki odtwarzacza
    audioPlayer.style.display = 'block';
}


// Funkcja do wy≈õwietlania pustych fal d≈∫wiƒôkowych
function drawIdleWaveform() {
    const canvasCtx = waveform.getContext('2d');
    let phase = 0;

    function draw() {
        animationFrameId = requestAnimationFrame(draw);

        // T≈Ço i wymiary
        canvasCtx.fillStyle = 'black'; // Czarny kolor t≈Ça
        canvasCtx.fillRect(0, 0, waveform.width, waveform.height);

        // Parametry linii
        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = 'white'; // U≈ºycie bia≈Çego koloru jak w visualizeWaveform

        canvasCtx.beginPath();

        // Rysowanie sinusoidalnych fal
        const sliceWidth = waveform.width / 50;
        let x = 0;

        for (let i = 0; i <= 50; i++) {
            const y = waveform.height / 2 + Math.sin(phase + i * 0.5) * (waveform.height / 4); // Skala dopasowana do wysoko≈õci
            if (i === 0) {
                canvasCtx.moveTo(x, y);
            } else {
                canvasCtx.lineTo(x, y);
            }
            x += sliceWidth;
        }

        canvasCtx.lineTo(waveform.width, waveform.height / 2);
        canvasCtx.stroke();

        phase += 0.1; // Sta≈Ça faza animacji
    }

    draw();
}


// Funkcja do wizualizacji fal d≈∫wiƒôkowych podczas nagrywania
function visualizeWaveform(stream) {
    audioContext = new AudioContext();
    const source = audioContext.createMediaStreamSource(stream);
    analyser = audioContext.createAnalyser();

    analyser.fftSize = 256;
    source.connect(analyser);

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);
    const canvasCtx = waveform.getContext('2d');

    function draw() {
        animationFrameId = requestAnimationFrame(draw);
        analyser.getByteTimeDomainData(dataArray);

        canvasCtx.fillStyle = 'black';
        canvasCtx.fillRect(0, 0, waveform.width, waveform.height);

        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = 'white';

        canvasCtx.beginPath();

        const sliceWidth = waveform.width / bufferLength;
        let x = 0;

        for (let i = 0; i < bufferLength; i++) {
            const v = dataArray[i] / 128.0;
            const y = (v * waveform.height) / 2;

            if (i === 0) {
                canvasCtx.moveTo(x, y);
            } else {
                canvasCtx.lineTo(x, y);
            }

            x += sliceWidth;
        }

        canvasCtx.lineTo(waveform.width, waveform.height / 2);
        canvasCtx.stroke();
    }

    draw();
}

// Funkcja konwertujƒÖca AudioBuffer na WAV
function audioBufferToWavBlob(buffer) {
    const numOfChannels = buffer.numberOfChannels;
    const sampleRate = buffer.sampleRate;
    const length = buffer.length * numOfChannels * 2 + 44;
    const arrayBuffer = new ArrayBuffer(length);
    const view = new DataView(arrayBuffer);

    let offset = 0;

    // Funkcje pomocnicze do pisania w nag≈Ç√≥wku WAV
    const writeString = (str) => {
        for (let i = 0; i < str.length; i++) {
            view.setUint8(offset++, str.charCodeAt(i));
        }
    };
    const writeUint16 = (value) => {
        view.setUint16(offset, value, true);
        offset += 2;
    };
    const writeUint32 = (value) => {
        view.setUint32(offset, value, true);
        offset += 4;
    };

    // Tworzenie nag≈Ç√≥wka WAV
    writeString('RIFF'); // RIFF chunk descriptor
    writeUint32(length - 8); // File size - 8 bytes
    writeString('WAVE'); // Format
    writeString('fmt '); // Subchunk1 ID
    writeUint32(16); // Subchunk1 size (16 for PCM)
    writeUint16(1); // Audio format (1 for PCM)
    writeUint16(numOfChannels); // Number of channels
    writeUint32(sampleRate); // Sample rate
    writeUint32(sampleRate * numOfChannels * 2); // Byte rate
    writeUint16(numOfChannels * 2); // Block align
    writeUint16(16); // Bits per sample
    writeString('data'); // Subchunk2 ID
    writeUint32(buffer.length * numOfChannels * 2); // Subchunk2 size

    // Dane audio
    const interleaved = interleave(buffer);
    for (let i = 0; i < interleaved.length; i++, offset += 2) {
        const sample = Math.max(-1, Math.min(1, interleaved[i]));
        view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
    }

    return new Blob([arrayBuffer], { type: 'audio/wav' });
}

// Funkcja interleavingu dla wielokana≈Çowego audio
function interleave(buffer) {
    const numOfChannels = buffer.numberOfChannels;
    const length = buffer.length;
    const interleaved = new Float32Array(length * numOfChannels);

    let inputIndex = 0;
    for (let i = 0; i < length; i++) {
        for (let channel = 0; channel < numOfChannels; channel++) {
            interleaved[inputIndex++] = buffer.getChannelData(channel)[i];
        }
    }
    return interleaved;
}

// Obs≈Çuga nagrywania i przesy≈Çania pliku
recordButton.addEventListener('click', async () => {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorder = new MediaRecorder(stream);
    audioChunks = [];

    // Rozpocznij wizualizacjƒô d≈∫wiƒôku
    visualizeWaveform(stream);

    mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) audioChunks.push(event.data);
    };

    mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        const audioContext = new AudioContext();
        const audioBuffer = await audioContext.decodeAudioData(await audioBlob.arrayBuffer());
        const wavBlob = audioBufferToWavBlob(audioBuffer);
        await uploadAndProcessAudio(wavBlob);

        // Zatrzymaj wizualizacjƒô i wr√≥ƒá do fal idle
        cancelAnimationFrame(animationFrameId);
        drawIdleWaveform();
    };

    mediaRecorder.start();
    recordButton.style.display = 'none';
    stopButton.style.display = 'block';
});

stopButton.addEventListener('click', () => {
    if (mediaRecorder && mediaRecorder.state !== "inactive") {
        mediaRecorder.stop();
        stopButton.style.display = 'none';
        recordButton.style.display = 'block';
        cancelAnimationFrame(animationFrameId);
        drawIdleWaveform();
    }
});



function showEmotionReaction(emotion) {
    const overlay = document.getElementById('emotion-overlay');
    const emotionImage = document.getElementById('emotion-image');

    // Ustaw ikonƒô/emotkƒô w overlay
    const emotionIcons = {
        happy: "üòÄ",
        sad: "üò¢",
        angry: "üò°",
        fear: "üò®",
        disgust: "ü§¢",
        neutral: "üòê",
        surprise: "üò≤"
    };

    emotionImage.textContent = emotionIcons[emotion] || "‚ùì";

    // Poka≈º reakcjƒô
    overlay.style.display = 'flex';
    setTimeout(() => {
        overlay.style.display = 'none';
    }, 2500); // 2.5 sekundy
}

const emotionResponses = {
    happy: [
        "Jeste≈õ radosny - Ciesz siƒô tƒÖ chwilƒÖ, to wspaniale czuƒá szczƒô≈õcie!",
        "Jeste≈õ radosny - Twoja rado≈õƒá jest zara≈∫liwa! Podziel siƒô niƒÖ z innymi.",
        "Jeste≈õ radosny - U≈õmiechnij siƒô jeszcze szerzej ‚Äì to Tw√≥j moment!",
        "Jeste≈õ radosny - To idealny czas na co≈õ, co kochasz robiƒá.",
        "Jeste≈õ radosny - Rado≈õƒá to energia ‚Äì wykorzystaj jƒÖ na co≈õ ekscytujƒÖcego."
    ],
    sad: [
        "Jeste≈õ smutny - Mo≈ºe ulubiona ksiƒÖ≈ºka lub piosenka poprawi Ci nastr√≥j?",
        "Jeste≈õ smutny - Czasem spacer w spokojnym miejscu pomaga znale≈∫ƒá ukojenie.",
        "Jeste≈õ smutny - Spr√≥buj zapisaƒá swoje my≈õli ‚Äì to pomaga spojrzeƒá na nie inaczej.",
        "Jeste≈õ smutny - Chwile smutku bywajƒÖ trudne, ale zawsze prowadzƒÖ do czego≈õ lepszego.",
        "Jeste≈õ smutny - Wybierz co≈õ, co zawsze Ciƒô rozlu≈∫nia ‚Äì ciep≈Ça herbata, koc i film?"
    ],
    angry: [
        "Jeste≈õ z≈Çy - We≈∫ kilka g≈Çƒôbokich oddech√≥w ‚Äì pomo≈ºe Ci to siƒô uspokoiƒá.",
        "Jeste≈õ z≈Çy - Mo≈ºe szybki spacer lub kilka ƒáwicze≈Ñ pomo≈ºe pozbyƒá siƒô napiƒôcia.",
        "Jeste≈õ z≈Çy - Pos≈Çuchaj g≈Ço≈õnej muzyki ‚Äì czasem wyra≈ºenie z≈Ço≈õci przez d≈∫wiƒôk dzia≈Ça.",
        "Jeste≈õ z≈Çy - Zapisz, co Ciƒô zdenerwowa≈Ço ‚Äì to mo≈ºe daƒá jasny obraz sytuacji.",
        "Jeste≈õ z≈Çy - Zr√≥b co≈õ, co pozwala Ci siƒô skupiƒá ‚Äì rysowanie, uk≈Çadanie puzzli czy gotowanie."
    ],
    fear: [
        "Boisz siƒô - We≈∫ g≈Çƒôboki oddech i skup siƒô na jednej ma≈Çej rzeczy w pokoju ‚Äì to pomaga wr√≥ciƒá do r√≥wnowagi.",
        "Boisz siƒô - Spr√≥buj przypomnieƒá sobie sytuacje, w kt√≥rych ju≈º przezwyciƒô≈ºy≈Çe≈õ strach.",
        "Boisz siƒô - Wyobra≈∫ sobie, ≈ºe sytuacja, kt√≥rej siƒô boisz, ju≈º minƒô≈Ça ‚Äì to tylko chwilowy stan.",
        "Boisz siƒô - Spokojna muzyka lub ciep≈Çe ≈õwiat≈Ço mo≈ºe pom√≥c z≈Çagodziƒá napiƒôcie.",
        "Boisz siƒô - Zr√≥b co≈õ ma≈Çego, co odwr√≥ci uwagƒô ‚Äì napij siƒô wody lub przejd≈∫ kilka krok√≥w."
    ],
    disgust: [
        "Jeste≈õ zniesmaczony - Spr√≥buj pomy≈õleƒá o czym≈õ zupe≈Çnie innym ‚Äì piƒôknym widoku lub ulubionym smaku.",
        "Jeste≈õ zniesmaczony - Umie≈õƒá odczucie w skali ‚Äì jak silne jest? To pomo≈ºe odzyskaƒá kontrolƒô.",
        "Jeste≈õ zniesmaczony - We≈∫ g≈Çƒôboki oddech i przypomnij sobie co≈õ, co sprawia Ci przyjemno≈õƒá.",
        "Jeste≈õ zniesmaczony - Zmiana otoczenia lub w≈ÇƒÖczenie ≈õwie≈ºego zapachu mo≈ºe pom√≥c zapomnieƒá o wstrƒôcie.",
        "Jeste≈õ zniesmaczony - Czysta przestrze≈Ñ wok√≥≈Ç mo≈ºe z≈Çagodziƒá ten odruch ‚Äì spr√≥buj uporzƒÖdkowaƒá co≈õ obok siebie."
    ],
    neutral: [
        "Jeste≈õ neutralny - Idealny moment, by odpoczƒÖƒá i z≈Çapaƒá r√≥wnowagƒô.",
        "Jeste≈õ neutralny - Czy wiesz, ≈ºe kosmos jest ca≈Çkowicie cichy? ≈ªadnego d≈∫wiƒôku!",
        "Jeste≈õ neutralny - Spok√≥j to si≈Ça. Mo≈ºe to czas na drobnƒÖ refleksjƒô?",
        "Jeste≈õ neutralny - To ≈õwietny moment, by przeczytaƒá co≈õ inspirujƒÖcego.",
        "Jeste≈õ neutralny - Zr√≥b co≈õ mi≈Çego dla siebie. Nawet drobne rzeczy majƒÖ znaczenie."
    ]
};

const emotionUnicode = {
    happy: "üòÄ",
    sad: "üò¢",
    angry: "üò°",
    fear: "üò®",
    disgust: "ü§¢",
    neutral: "üòê",
    surprise: "üò≤"
};

// Funkcja do syntezowania mowy
function speakResponse(text) {
    const synth = window.speechSynthesis;

    if (!synth) {
        console.error('Speech synthesis not supported in this browser.');
        return;
    }

    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'pl-PL'; // Ustaw jƒôzyk na polski
    utterance.pitch = 1; // Ton g≈Çosu (1 to standardowy)
    utterance.rate = 1; // Szybko≈õƒá mowy (1 to standardowa)
    synth.speak(utterance);
}

// Funkcja aktualizacji odpowiedzi na podstawie emocji
function updateEmotionResponse(emotion) {
    const responseContainer = document.getElementById('emotion-response');
    const emojiContainer = document.getElementById('response-emoji');

    let randomResponse;

    if (emotionResponses[emotion]) {
        // Wybierz losowƒÖ odpowied≈∫
        randomResponse = emotionResponses[emotion][Math.floor(Math.random() * emotionResponses[emotion].length)];
        responseContainer.textContent = randomResponse; // Wy≈õwietl odpowied≈∫
    } else {
        randomResponse = "Nieznana emocja.";
        responseContainer.textContent = randomResponse;
    }

    // Ustaw emotkƒô
    emojiContainer.textContent = emotionUnicode[emotion] || "‚ùì";

    // Wy≈ÇƒÖcz odczytanie odpowiedzi na g≈Ços
    // speakResponse(randomResponse); // Zakomentuj lub usu≈Ñ tƒô liniƒô, aby wy≈ÇƒÖczyƒá d≈∫wiƒôk
}







async function uploadAndProcessAudio(blob) {
    const formData = new FormData();
    formData.append('audio_data', blob, 'recording.wav');

    try {
        const response = await fetch('/process_audio_combined', {
            method: 'POST',
            body: formData,
        });
        const data = await response.json();

        if (data.error) {
            // Wy≈õwietl b≈ÇƒÖd w odpowiednich miejscach
            textDisplay.textContent = `Error: ${data.error}`;
            document.getElementById('text-emotions').textContent = 'Error occurred.';
            document.getElementById('audio-emotions').textContent = 'Error occurred.';
        } else {
            // Wy≈õwietl transkrypcjƒô
            textDisplay.textContent = data.transcription || 'No text recognized';

            // Wy≈õwietl emocje z tex_rec.py (tekst)
            const textEmotionHTML = Object.entries(data.tex_rec_emotions || {}).map(
                ([emotion, value]) => `${emotion}: ${value.toFixed(2)}%`
            ).join('<br>');
            document.getElementById('text-emotions').innerHTML = textEmotionHTML;

            // Wy≈õwietl emocje z new_file_emotion.py (d≈∫wiƒôk)
            const audioEmotionHTML = Object.entries(data.audio_emotions || {}).map(
                ([emotion, value]) => `${emotion}: ${value.toFixed(2)}%`
            ).join('<br>');
            document.getElementById('audio-emotions').innerHTML = audioEmotionHTML;

            // Wy≈õwietl ostateczne emocje
            const finalEmotionHTML = Object.entries(data.final_emotions || {}).map(
                ([emotion, value]) => `${emotion}: ${value.toFixed(2)}%`
            ).join('<br>');
            document.getElementById('final-emotions').innerHTML = finalEmotionHTML;

            // Wy≈õwietl rozpoznanƒÖ emocjƒô
            const yourMood = document.getElementById('your-mood');
            if (data.recognized_emotion) {
                yourMood.textContent = `Recognized Emotion: ${data.recognized_emotion}`;

                // Zaktualizuj odpowied≈∫ i odczytaj jƒÖ na g≈Ços
                updateEmotionResponse(data.recognized_emotion);
            } else {
                yourMood.textContent = `Recognized Emotion: No emotion detected`;
                speakResponse("Nie wykryto emocji.");
            }

            // Ustaw plik do ods≈Çuchania
            setAudioPlayer(URL.createObjectURL(blob));
        }
    } catch (error) {
        // Obs≈Çu≈º b≈ÇƒÖd
        textDisplay.textContent = `Processing failed: ${error.message}`;
        document.getElementById('text-emotions').textContent = 'Error occurred.';
        document.getElementById('audio-emotions').textContent = 'Error occurred.';
        console.error('Error processing audio:', error);
    }
}


// Rozpocznij wizualizacjƒô pustych fal
drawIdleWaveform();

